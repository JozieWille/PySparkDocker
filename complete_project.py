# -*- coding: utf-8 -*-
"""Complete_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mdHSaBlZuzaDQaY88EVHR6tt81KXcIPn
"""

from pyspark.sql import SparkSession
import pandas as pd
import numpy as np
spark = SparkSession.builder.master('local[*]').config("spark.driver.memory", "2g").appName('Fraud_Detection').getOrCreate()
df = spark.read.options(header=True, nullValue='NA', inferSchema=True).csv('./Fraud_Detection.csv')
df = df.dropDuplicates()

df.head()

df.count()

#creating a destination column for customer and merchant 
from pyspark.sql.functions import when, col

df = df.withColumn("Type_Dest", when(col("nameDest").startswith("C"), "Customer_Dest").when(col("nameDest").startswith("M"), "Merchant_Dest").otherwise("Other"))

#creating an original column for customer/merchant 
from pyspark.sql.functions import when, col
df = df.withColumn("Type_Orig", when(col("nameOrig").startswith("C"), "Customer_Orig").when(col("nameOrig").startswith("M"), "Merchant_Orig").otherwise("Other"))

df.head()

from pyspark.sql.functions import corr
corr_value = df.select(corr('isFraud', 'isFlaggedFraud')).collect()[0][0]
print(corr_value)

from pyspark.sql.functions import sum as pyspark_sum

fraud_sum = df.select(pyspark_sum('isFraud')).collect()[0][0]

print(fraud_sum)

flagged_sum = df.select(pyspark_sum('isFlaggedFraud')).collect()[0][0]
print(flagged_sum)

"""#Creating new hour/day/time of day columns"""

# looking at the max step 
from pyspark.sql.functions import max

max_step = df.agg(max("step")).collect()[0][0]
print("The maximum value in the 'step' column is:", max_step)

# looking at the min step
from pyspark.sql.functions import min

min_step = df.agg(min("step")).collect()[0][0]
print("The minimum value in the 'step' column is:", min_step)

# null/missing values
from pyspark.sql.functions import count, when, col

# Count the number of null values in each column of df3
null_counts = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])

# Convert the result to a dictionary for easy access
null_counts_dict = null_counts.first().asDict()

# Print the number of null values in each column
for col_name, null_count in null_counts_dict.items():
    print(f"Column '{col_name}' has {null_count} null values.")

# code to calculate time of day
from pyspark.sql.functions import when

df = df.withColumn("hour_of_day", (df.step - 1) % 24)  # Calculate the hour of the day

from pyspark.sql.functions import floor

df = df.withColumn("day", floor((df.step - 1) / 24) + 1)  # Calculate the day number

df = df.withColumn("time_of_day", 
                   when((df.hour_of_day >= 5) & (df.hour_of_day < 12), "morning")
                   .when((df.hour_of_day >= 12) & (df.hour_of_day < 17), "afternoon")
                   .when((df.hour_of_day >= 17) & (df.hour_of_day < 21), "evening")
                   .otherwise("night"))  # Assign labels based on hour of the day

df.head(2)

"""#Querying to look at distributions of fraud"""

df.createOrReplaceTempView("my_table")

# execute a SQL query to group by time_of_day and count the number of fraud transactions
fraud_by_ToD = spark.sql("""
    SELECT (time_of_day) as ToD, sum(isFraud) as fraud_count
    FROM my_table
    GROUP BY time_of_day order by fraud_count
""")

fraud_by_ToD.show()

# Convert the PySpark DataFrame to a Pandas DataFrame
pandas_df = fraud_by_ToD.toPandas()
pandas_df

# Create a bar plot using Matplotlib
plt.bar(pandas_df['ToD'], pandas_df['fraud_count'])
plt.xlabel('Time of Day')
plt.ylabel('Fraud Count')
plt.title('Fraud Count by Time of Day')
plt.show()

#Fraud rates by time of day 
avg_fraud_by_ToD = spark.sql("""
    SELECT (time_of_day) as ToD, avg(isFraud) as fraud_avg
    FROM my_table
    GROUP BY time_of_day order by fraud_avg
""")

avg_fraud_by_ToD.show()

pandas_df2 = avg_fraud_by_ToD.toPandas()
pandas_df2

plt.bar(pandas_df2['ToD'], pandas_df2['fraud_avg'])
plt.xlabel('Time of Day')
plt.ylabel('Average Fraud')
plt.title('Average Fraud by Time of Day')
plt.show()

# execute a SQL query to group by day and count the number of fraud transactions
fraud_by_day = spark.sql("""
    SELECT day, sum(isFraud) as fraud_count
    FROM my_table
    GROUP BY day order by fraud_count
""")

fraud_by_day.show(31)

pandas_df3 = fraud_by_day.toPandas()
pandas_df3

plt.bar(pandas_df3['day'], pandas_df3['fraud_count'], color = 'red')
plt.xlabel('Day')
plt.ylabel('Fraud Count')
plt.title('Fraud Count by Day of Month')
plt.show()

avg_fraud_by_day = spark.sql("""
    SELECT day, avg(isFraud) as fraud_avg
    FROM my_table
    GROUP BY day order by fraud_avg
""")

avg_fraud_by_day.show(31)

pandas_df4 = avg_fraud_by_day.toPandas()
pandas_df4

plt.bar(pandas_df4['day'], pandas_df4['fraud_avg'], color = 'red')
plt.xlabel('Day')
plt.ylabel('Average Fraud')
plt.title('Average Fraud by Day of Month')
plt.show()

"""Preparing the data for random forest"""

df.printSchema()

unique_vals = df.select('type').distinct().collect()
for val in unique_vals:
    print(val[0])

from pyspark.sql.functions import col, when, regexp_extract, trim
from pyspark.sql.types import DoubleType, IntegerType

# Convert categorical columns to numerical values using one-hot encoding
df = df.select('*', *[when(col('type') == val, 1).otherwise(0).alias(val) for val in ['CASH_OUT', 'CASH_IN', 'TRANSFER', 'DEBIT', 'PAYMENT']])
df = df.select('*', *[when(col('Type_Orig') == val, 1).otherwise(0).alias(val) for val in ['Merchant_Orig', 'Customer_Orig']])
df = df.select('*', *[when(col('Type_Dest') == val, 1).otherwise(0).alias(val) for val in ['Merchant_Dest', 'Customer_Dest']])
df = df.select('*', *[when(col('time_of_day') == val, 1).otherwise(0).alias(val) for val in ['morning', 'afternoon', 'evening', 'night']])
df = df.drop('type', 'Type_Orig', 'Type_Dest', 'time_of_day')

# Extract numeric value from nameOrig and nameDest columns
df = df.withColumn('nameOrig', regexp_extract(trim(col('nameOrig')), '\d+', 0).cast(IntegerType()))
df = df.withColumn('nameDest', regexp_extract(trim(col('nameDest')), '\d+', 0).cast(IntegerType()))

df.printSchema()

df.head(5)

#remove the step column
df = df.drop("step")

"""Splitting data into train and test"""

#training = first 20 days, testing = last 11 days 
train = df.filter(df.day <= 20)
test = df.filter(df.day > 20)

"""#Exploding the Training Data"""

from pyspark.sql.functions import col, explode, array, lit

major_df = train.filter(col("isFraud") == 0)
minor_df = train.filter(col("isFraud") == 1)
ratio = int(major_df.count()/minor_df.count())
print("ratio: {}".format(ratio))

a = range(ratio)
# duplicate the minority rows
oversampled_df = minor_df.withColumn("dummy", explode(array([lit(x) for x in a]))).drop('dummy')

# combine both oversampled minority rows and previous majority rows 
train = major_df.unionAll(oversampled_df)
train.show()

#checking new ratio 
major_df1 = train.filter(col("isFraud") == 0)
minor_df1 = train.filter(col("isFraud") == 1)
ratio = int(major_df1.count()/minor_df1.count())
print("ratio: {}".format(ratio))

#checking fraud occurrence different method 
from pyspark.sql.functions import avg as pyspark_avg

train_fraud_avg = train.select(pyspark_avg('isFraud')).collect()[0][0]
print(train_fraud_avg)

#counting rows in training and testing df 
num_train = train.count()
num_test = test.count()
print(num_train)
print(num_test)

#Creating pie charts of fraudulent transactions before and after oversampling 
train_fc = train.groupBy('isFraud').count().toPandas()

# Create a pie chart using matplotlib
plt.pie(train_fc['count'], labels=train_fc['isFraud'], autopct='%1.1f%%')
plt.title('Fraudulent Transactions After Oversampling')
plt.axis('equal')
plt.show()

import matplotlib.pyplot as plt
import pyspark.sql.functions as fn

fraud_counts = df.groupBy("isFraud").count().withColumnRenamed("count", "num_transactions")

# Collect the counts as a list
counts = fraud_counts.select("num_transactions").rdd.flatMap(lambda x: x).collect()

# Create the labels for the pie chart
labels = ["Non-Fraudulent", "Fraudulent"]

# Define the color palette
colors = ["#FFA07A", "#1E90FF"]  # Switched colors

# Create the pie chart
plt.pie(counts, labels=labels, colors=colors, autopct='%1.1f%%')

plt.title('Fraudulent Transactions Before Oversampling')

# Show the plot
plt.show()

"""#Building a random forest model: """

train.columns

"""Random Forest Classifier"""

# random forest
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.feature import VectorAssembler

# Define the input features

input_cols = ['amount',
'nameOrig',
 'oldbalanceOrg',
 'newbalanceOrig',
 'nameDest',
 'oldbalanceDest',
 'newbalanceDest',
 'isFlaggedFraud',
 'hour_of_day',
 'day',
 'CASH_OUT',
 'CASH_IN',
 'TRANSFER',
 'DEBIT',
 'PAYMENT',
 'Merchant_Orig',
 'Customer_Orig',
 'Merchant_Dest',
 'Customer_Dest',
 'morning',
 'afternoon',
 'evening',
 'night']

# Create a vector assembler to combine the input features into a single vector column
assembler = VectorAssembler(inputCols=input_cols, outputCol='features')

# Apply the vector assembler to the training and testing data
train_data = assembler.transform(train).select('features', 'isFraud')
test_data = assembler.transform(test).select('features', 'isFraud')

# Create a random forest classifier
rf = RandomForestClassifier(labelCol='isFraud', featuresCol='features', numTrees=10, maxDepth=10)

# Train the random forest model on the training data
model = rf.fit(train_data)

# Use the trained model to make predictions on the testing data
predictions = model.transform(test_data)

# Evaluate the accuracy of the model
from pyspark.ml.evaluation import BinaryClassificationEvaluator
evaluator = BinaryClassificationEvaluator(labelCol='isFraud', metricName='areaUnderROC')
accuracy = evaluator.evaluate(predictions)
print('Model accuracy:', accuracy)

# Get feature importance from the trained model
importances = model.featureImportances

# Create a list of tuples containing feature names and their importances
feature_importances = [(col, imp) for col, imp in zip(input_cols, importances)]

# Sort the list of feature importances in descending order by importance score
sorted_importances = sorted(feature_importances, key=lambda x: x[1], reverse=True)

# Print feature importance scores in descending order
for i in range(len(sorted_importances)):
    print(sorted_importances[i][0], ':', sorted_importances[i][1])

from pyspark.ml.evaluation import MulticlassClassificationEvaluator

# Evaluate precision and recall using MulticlassClassificationEvaluator
evaluator = MulticlassClassificationEvaluator(labelCol='isFraud', predictionCol='prediction', metricName='weightedPrecision')
precision = evaluator.evaluate(predictions)
evaluator = MulticlassClassificationEvaluator(labelCol='isFraud', predictionCol='prediction', metricName='weightedRecall')
recall = evaluator.evaluate(predictions)

# Print precision and recall
print('Precision:', precision)
print('Recall:', recall)

from pyspark.ml.evaluation import MulticlassClassificationEvaluator

# Create a multiclass classification evaluator
evaluator = MulticlassClassificationEvaluator(labelCol='isFraud', predictionCol='prediction', metricName='accuracy')

# Compute the confusion matrix
confusion_matrix = predictions.groupBy('isFraud', 'prediction').count().orderBy('isFraud', 'prediction')

# Print the confusion matrix
confusion_matrix.show()